# RWKV-T 19M B200 Training Configuration

# Model Architecture
model:
  dim: 256
  n_enc: 12
  n_pred: 4
  vocab_size: 2049  # 2048 BPE + 1 blank
  dropout: 0.1

# Training Hyperparameters
train:
  batch_size: 2      # Optimized for B200 (141GB)
  accum_steps: 32    # 32 * 32 = 1024 effective batch size
  lr: 1e-4
  epochs: 25
  weight_decay: 0.1
  grad_clipping: 1.0
  seed: 42
  num_workers: 12    # High throughput for B200 CPU
  max_audio_duration: 30.0 # Full LibriSpeech coverage for B200
  

# Metadata & Monitoring
monitoring:
  wandb_project: "rwkv-t-asr"
  log_interval: 10
  validation_hook_interval: 1000
  num_validation_samples: 5
  export_manifest: true

# Path Configuration
paths:
  tokenizer_model: "bpe.model"
  checkpoint_dir: "checkpoints"
  log_file: "logs/training_log.csv"
  manifest_file: "logs/experiment_manifest.json"
